#  examplePar04_onnx.in
#
# Detector Construction 
/Par04/detector/setDetectorInnerRadius 80 cm
/Par04/detector/setDetectorLength 2 m
/Par04/detector/setNbOfLayers 90
/Par04/detector/setAbsorber 0 G4_W 1.4 mm true
/Par04/detector/setAbsorber 1 G4_Si 0.3 mm true
## 2.325 mm of tungsten =~ 0.25 * 9.327 mm = 0.25 * R_Moliere
/Par04/mesh/setSizeOfRhoCells 2.325 mm
## 2 * 1.4 mm of tungsten =~ 0.65 X_0
/Par04/mesh/setSizeOfZCells 3.4 mm
/Par04/mesh/setNbOfRhoCells 18
/Par04/mesh/setNbOfPhiCells 50
/Par04/mesh/setNbOfZCells 45

# Initialize
/run/numberOfThreads 1
/run/initialize

/gun/energy 10 GeV
/gun/position 0 0 0
/gun/direction 0 1 0

# Inference Setup
## dimension of the latent vector (encoded vector in a Variational Autoencoder model)
/Par04/inference/setSizeLatentVector 10
## size of the condition vector (energy, angle and geometry)  
/Par04/inference/setSizeConditionVector 4
## path to the model which is set to download by cmake
/Par04/inference/setModelPathName ./MLModels/Generator.onnx
/Par04/inference/setProfileFlag 1
/Par04/inference/setOptimizationFlag 0
/Par04/inference/setDnnlFlag 0
/Par04/inference/setOpenVinoFlag 1
/Par04/inference/setCudaFlag 0
/Par04/inference/setTensorrtFlag 0
/Par04/inference/setInferenceLibrary ONNX

# onednn options
/Par04/inference/onednn/setEnableCpuMemArena 1

# cuda options
/Par04/inference/cuda/setDeviceId 0
/Par04/inference/cuda/setGpuMemLimit 2147483648
/Par04/inference/cuda/setArenaExtendedStrategy kSameAsRequested
/Par04/inference/cuda/setCudnnConvAlgoSearch DEFAULT
/Par04/inference/cuda/setDoCopyInDefaultStream 1
/Par04/inference/cuda/setCudnnConvUseMaxWorkspace 1

# tensorrt options
/Par04/inference/trt/setDeviceId 0
/Par04/inference/trt/setMaxWorkspaceSize 2147483648
/Par04/inference/trt/setMaxPartitionIterations 10
/Par04/inference/trt/setMinSubgraphSize 5
/Par04/inference/trt/setFp16Enable 0
/Par04/inference/trt/setInt8Enable 0
/Par04/inference/trt/etInt8UseNativeCalibrationTable 1
/Par04/inference/trt/setEngineCacheEnable 1
/Par04/inference/trt/setEngineCachePath /opt/trt/geant4/cache
/Par04/inference/trt/setDumpSubgraphs 1

# openvino options
/Par04/inference/openvino/setDeviceType CPU_FP32
/Par04/inference/openvino/setEnableVpuFastCompile 0
#/Par04/inference/openvino/setDeviceId 
/Par04/inference/openvino/setNumOfThreads 1
/Par04/inference/openvino/setUseCompiledNetwork 0
#/Par04/inference/openvino/setBlobDumpPath 
/Par04/inference/openvino/setEnableOpenCLThrottling 0

## set mesh size for inference == mesh size of a full sim that
## was used for training; it coincides with readout mesh size
/Par04/inference/setSizeOfRhoCells 2.325 mm
/Par04/inference/setSizeOfZCells 3.4 mm
/Par04/inference/setNbOfRhoCells 18
/Par04/inference/setNbOfPhiCells 50
/Par04/inference/setNbOfZCells 45

# Fast Simulation
/analysis/setFileName 10GeV_100events_fastsim_onnx.root
## dynamically set readout mesh from particle direction
## needs to be the first fast sim model!
/param/ActivateModel defineMesh
## ML fast sim, configured with the inference setup /Par04/inference
/param/ActivateModel inferenceModel
/run/beamOn 999
